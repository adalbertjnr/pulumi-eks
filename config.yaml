spec:
  networking:
    name: apps-vpc
    cidrBlock: "10.0.0.0/16"
    subnets:
    - name: apps-subnet-1a-priv
      cidrBlock: "10.0.0.0/24"
      publicIpOnLaunch: false
      availabilityZone: "us-east-1a"
      tags:
        "kubernetes.io/role/internal-elb": 1
    - name: apps-subnet-1b-priv
      cidrBlock: "10.0.1.0/24"
      publicIpOnLaunch: false
      availabilityZone: "us-east-1b"
      tags:
        "kubernetes.io/role/internal-elb": 1

    - name: apps-subnet-1a-pub
      cidrBlock: "10.0.2.0/24"
      publicIpOnLaunch: true
      availabilityZone: "us-east-1a"
      tags:
        "kubernetes.io/role/elb": 1
    - name: apps-subnet-1b-pub
      cidrBlock: "10.0.3.0/24"
      publicIpOnLaunch: true
      availabilityZone: "us-east-1b"
      tags:
        "kubernetes.io/role/elb": 1

  cluster:
    name: cluster-test-new
    environment: dev
    region: us-east-1
    kubernetesVersion: "1.31"
    vpcId: apps-vpc
    subnets: ["apps-subnet-1a-pub", "apps-subnet-1b-pub"]
    securityGroups: ["sg-102930sdccc", "sg-102390c0s"]

  nodeGroups:
  - name: ng-dev-test
    scalingConfig:
      minSize: 1
      desiredSize: 1
      maxSize: 1
    instanceType: t3.medium
    imageId: "ami-0181ca43ef1eba8ed"
    nodeLabels:
      apps: pulumi-app

  identityPodAgent:
   deploy: true
   identities:
    roles:
    - roleName: eks-pod-example
      awsPolicies: ["arn:aws:iam::aws:policy/AmazonS3FullAccess"]
      selfManagedPoliciesPath: ["../policies/identity_pod_policies/example-policy.json"]
    relationships:
    - roleName: eks-pod-example
      namespace: default
    - roleName: eks-pod-example
      namespace: example

  helmChartsComponentes:
    components:
    - name: aws-lb-controller
      chart: aws-load-balancer-controller
      skipCidrs: false
      version: "1.11.0"
      repository: "https://aws.github.io/eks-charts"
      namespace: "kube-system"
      withOidcProvider:
        create: true
        role:
          name: AWSLoadBalancerControllerRole
          awsPolicies: []
          selfManagedPoliciesPath: ["../policies/alb_policy/alb-policy.json"]
        serviceAccount:
          name: aws-load-balancer-controller
      setValues:
        clusterName: cluster-test-new
        controllerConfig:
          featureGates:
            SubnetsClusterTagCheck: false
        replicaCount: 1
        serviceAccount:
          create: false
          name: aws-load-balancer-controller

    - name: argo-cd
      chart: argo-cd
      createNamespace: true
      skipCidrs: false
      version: "7.7.22"
      repository: "https://argoproj.github.io/argo-helm"
      namespace: "argocd"
      setValues:
        server:
          ingress:
            enabled: true
            annotations:
              alb.ingress.kubernetes.io/certificate-arn: ""
              alb.ingress.kubernetes.io/scheme: internet-facing
              alb.ingress.kubernetes.io/target-type: ip
              alb.ingress.kubernetes.io/backend-protocol: HTTPS
              alb.ingress.kubernetes.io/ssl-policy: ELBSecurityPolicy-FS-1-2-Res-2020-10
              alb.ingress.kubernetes.io/ssl-redirect: "443"
              alb.ingress.kubernetes.io/group.name: my-app-group
              alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS": 443}]'
              kubernetes.io/ingress.class: alb
            ingressClassName: alb
            hostname: argocd.example.com